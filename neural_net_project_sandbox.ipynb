{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFB Machine Learning with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('power5.csv', sep=',',header=0)\n",
    "data = data.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "#for now, remove polls because this nn is not built to handle NaN values\n",
    "data = data.drop(columns = [\"H_Poll_curr\", \"H_Poll_pre\", \"H_Poll_high\",\"A_Poll_curr\", \"A_Poll_pre\", \"A_Poll_high\", \"Home_score\", \"Away_score\"])\n",
    "\n",
    "#remove and simplify unecessary data\n",
    "data = data.drop(columns = ['H_Overall_Pct', 'H_Conf_Pct', 'H_SOS', 'H_Offensive_Pass_Pct', 'H_Offensive_RusH_Yards_Average', \n",
    "                            'H_Offensive_Total_Average', 'H_Offensive_First_Downs_Total', 'H_Offensive_Turnovers_Total',\n",
    "                           'H_Defensive_Pass_Pct', 'H_Defensive_RusH_Yards_Average', 'H_Defensive_Total_Average',\n",
    "                           'H_Defensive_First_Downs_Total', 'H_Defensive_Turnovers_Total', \n",
    "                            'H_Offensive_Total_Plays', 'H_Offensive_Total_Yards',\n",
    "                           'H_Defensive_Total_Plays', 'H_Defensive_Total_Yards',\n",
    "                           'A_Overall_Pct', 'A_Conf_Pct', 'A_SOS', 'A_Offensive_Pass_Pct', 'A_Offensive_RusA_Yards_Average', \n",
    "                            'A_Offensive_Total_Average', 'A_Offensive_First_Downs_Total', 'A_Offensive_Turnovers_Total',\n",
    "                           'A_Defensive_Pass_Pct', 'A_Defensive_RusA_Yards_Average', 'A_Defensive_Total_Average',\n",
    "                           'A_Defensive_First_Downs_Total', 'A_Defensive_Turnovers_Total', \n",
    "                            'A_Offensive_Total_Plays', 'A_Offensive_Total_Yards',\n",
    "                           'A_Defensive_Total_Plays', 'A_Defensive_Total_Yards'])\n",
    "\n",
    "cols = list(data.columns.values) #Make a list of all of the columns in the df\n",
    "cols.pop(cols.index('Winner')) #Remove b from list\n",
    "data = data[cols+['Winner']] #Create new dataframe with columns in the order you want\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#.info()\n",
    "#cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier\n",
    "\n",
    "## Assign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign data for columns 2:109 to X \n",
    "X = data.iloc[:, 2:76]\n",
    "\n",
    "# assign y as winner\n",
    "y = data.iloc[:,76]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  \n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10, 10, 10, 10), max_iter=1000)  \n",
    "mlp.fit(X_train, y_train.values.ravel())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=cols, class_names=y_train, discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, X_test.shape[0])\n",
    "exp = explainer.explain_instance(X_test[i], mlp.predict_proba, num_features=2, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,predictions))  \n",
    "print(classification_report(y_test,predictions))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Conference Championships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pd.read_csv('conf_champs.csv', sep=',',header=0)\n",
    "conf = conf.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "#for now, remove polls because this nn is not built to handle NaN values\n",
    "conf = conf.drop(columns = [\"H_Poll_curr\", \"H_Poll_pre\", \"H_Poll_high\",\"A_Poll_curr\", \"A_Poll_pre\", \"A_Poll_high\"])\n",
    "\n",
    "#remove and simplify unecessary data\n",
    "conf = conf.drop(columns = ['H_Overall_Pct', 'H_Conf_Pct', 'H_SOS', 'H_Offensive_Pass_Pct', 'H_Offensive_RusH_Yards_Average', \n",
    "                            'H_Offensive_Total_Average', 'H_Offensive_First_Downs_Total', 'H_Offensive_Turnovers_Total',\n",
    "                           'H_Defensive_Pass_Pct', 'H_Defensive_RusH_Yards_Average', 'H_Defensive_Total_Average',\n",
    "                           'H_Defensive_First_Downs_Total', 'H_Defensive_Turnovers_Total', \n",
    "                            'H_Offensive_Total_Plays', 'H_Offensive_Total_Yards',\n",
    "                           'H_Defensive_Total_Plays', 'H_Defensive_Total_Yards',\n",
    "                           'A_Overall_Pct', 'A_Conf_Pct', 'A_SOS', 'A_Offensive_Pass_Pct', 'A_Offensive_RusA_Yards_Average', \n",
    "                            'A_Offensive_Total_Average', 'A_Offensive_First_Downs_Total', 'A_Offensive_Turnovers_Total',\n",
    "                           'A_Defensive_Pass_Pct', 'A_Defensive_RusA_Yards_Average', 'A_Defensive_Total_Average',\n",
    "                           'A_Defensive_First_Downs_Total', 'A_Defensive_Turnovers_Total', \n",
    "                            'A_Offensive_Total_Plays', 'A_Offensive_Total_Yards',\n",
    "                           'A_Defensive_Total_Plays', 'A_Defensive_Total_Yards'])\n",
    "\n",
    "\n",
    "conf_cols = list(conf.columns.values) #Make a list of all of the columns in the df\n",
    "#conf_cols.pop(conf_cols.index('Winner')) #Remove b from list\n",
    "#conf = conf[conf_cols+['Winner']] #Create new dataframe with columns in the order you want\n",
    "\n",
    "X_conf = conf.iloc[:, 2:76]\n",
    "X_c =  X_conf.values\n",
    "X_c = scaler.transform(X_c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_predictions = mlp.predict(X_c)\n",
    "conf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_conf_champs = np.array([0,0,1,0,1,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(real_conf_champs,conf_predictions))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Bowl Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl = pd.read_csv('bowl_champs.csv', sep=',',header=0)\n",
    "bowl = bowl.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "#for now, remove polls because this nn is not built to handle NaN values\n",
    "bowl = bowl.drop(columns = [\"H_Poll_curr\", \"H_Poll_pre\", \"H_Poll_high\",\"A_Poll_curr\", \"A_Poll_pre\", \"A_Poll_high\"])\n",
    "\n",
    "#remove and simplify unecessary data\n",
    "bowl = bowl.drop(columns = ['H_Overall_Pct', 'H_Conf_Pct', 'H_SOS', 'H_Offensive_Pass_Pct', 'H_Offensive_RusH_Yards_Average', \n",
    "                            'H_Offensive_Total_Average', 'H_Offensive_First_Downs_Total', 'H_Offensive_Turnovers_Total',\n",
    "                           'H_Defensive_Pass_Pct', 'H_Defensive_RusH_Yards_Average', 'H_Defensive_Total_Average',\n",
    "                           'H_Defensive_First_Downs_Total', 'H_Defensive_Turnovers_Total', \n",
    "                            'H_Offensive_Total_Plays', 'H_Offensive_Total_Yards',\n",
    "                           'H_Defensive_Total_Plays', 'H_Defensive_Total_Yards',\n",
    "                           'A_Overall_Pct', 'A_Conf_Pct', 'A_SOS', 'A_Offensive_Pass_Pct', 'A_Offensive_RusA_Yards_Average', \n",
    "                            'A_Offensive_Total_Average', 'A_Offensive_First_Downs_Total', 'A_Offensive_Turnovers_Total',\n",
    "                           'A_Defensive_Pass_Pct', 'A_Defensive_RusA_Yards_Average', 'A_Defensive_Total_Average',\n",
    "                           'A_Defensive_First_Downs_Total', 'A_Defensive_Turnovers_Total', \n",
    "                            'A_Offensive_Total_Plays', 'A_Offensive_Total_Yards',\n",
    "                           'A_Defensive_Total_Plays', 'A_Defensive_Total_Yards'])\n",
    "\n",
    "\n",
    "bowl_cols = list(bowl.columns.values) #Make a list of all of the columns in the df\n",
    "#bowl_cols.pop(bowl_cols.index('Winner')) #Remove b from list\n",
    "#bowl = conf[bowl_cols+['Winner']] #Create new dataframe with columns in the order you want\n",
    "\n",
    "X_bowl = bowl.iloc[:, 2:76]\n",
    "X_b =  X_bowl.values\n",
    "X_b = scaler.transform(X_b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowl_predictions = mlp.predict(X_b)\n",
    "bowl_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,clf_predictions))  \n",
    "print(classification_report(y_test,clf_predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,rf_predictions))  \n",
    "print(classification_report(y_test,rf_predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
